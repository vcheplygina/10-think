\documentclass[10pt,letterpaper]{article}
\include{settings}

%% Define per-paper macros.
\newcommand{\rulemajor}[1]{\section{#1}}
\begin{document}
\vspace*{0.2in}

\begin{flushleft}
{\Large
\textbf\newline{Ten Simple Rules for Thinking Like a Programmer}
}
\newline
\\
{Greg~Wilson}\textsuperscript{1,*}
\\
\textbf{1} RStudio, Inc. / greg.wilson@rstudio.com
\\
\bigskip
* Corresponding author.
\end{flushleft}

\section*{Abstract}

We describe ten characteristic ways in which programmers view the world that are
often useful in other contexts.

\section*{Author Summary}

The term ``computational thinking'' is widely used, but there is little
consensus on what it actually encompasses.  Experienced programmers, on the
other hand, tend to think about problems in similar ways, and understanding
these ways can help people in other domains see their own fields with fresh
eyes.

\section*{Introduction}

The term ``computational thinking'' has been widely used since \cite{Wing2006}
introduced it more than a decade ago.  However, there is little consensus on
what it actually encompasses.  Experienced programmers, on the other hand,
tend to think about problems in similar ways, and understanding those ways
can help people in other domains---such as scientific research---see their
own fields with fresh eyes.

\subsection*{Acknowledgments}

An early version of this paper was inspired by Jon Udell's essay ``Seven Ways to
Think Like the Web'' \cite{Udel2011}.

\rulemajor{Programs are data.}

First, and most importantly, everything a computer works with is just 1's and
0's.  From email messages to pictures of far-off galaxies, it's all just bits.

This seems obvious, but one of its implications is that programs are just
another kind of data.  This is the key insight that all of modern computing is
built on.  The source code for a program is just a set of text files, no
different from a thesis or a poem.  Once that text is loaded into memory, the
instructions are just more bytes, and can be stored in data structures, passed
around, or altered.  Almost all advanced programming techniques, from callbacks
in JavaScript to decorators in Python and lazy evaluation in R, depend on this
fact in one way or another.

\rulemajor{Data must be interpreted.}

The complement to the first rule is that data doesn't mean anything on its
own---it has to be interpreted.  The bits 01100100011000010111100001100001 are
simultaneously the word ``data'', the integer 1,684,108,385, the floating-point
number 1.6635613602263159e+22, a bluish-gray pixel that's slightly transparent,
a medium-loud sound sample with slightly higher volume in the left channel, and
an instruction to multiply the value on the top of the stack by the value
beneath it and store the result in a particular location in memory.

A corollary is that machines don't understand: they only obey.  If you look at
this image, you see the word ``data'':

\includegraphics[width=5.0cm]{data.png}

A machine doesn't; it doesn't even see four blobs of blue pixels on a gray
background, because it doesn't ``see'' anything.  The computer stores this
image, and a program, as bits in memory.  If those bits happen to correspond to
instructions for the computer's CPU, and if those instructions happen to make up
a program for detecting shapes in images and matching those shapes with letters
in the alphabet, then the computer may very well output ``data'', but \emph{it
  doesn't understand}.  Calling a variable ``temperature'' doesn't mean the
computer will store a temperature in it---it would do exactly the same thing if
the variable was called ``pressure'' or ``frankenstein'' or ``a7''.  Again, this
may seem obvious, but thirty years after Pea first called it the ``superbug'',
believing that the computer will somehow understand intent remains a common
error \cite{Pea1986}.

\rulemajor{Nothing in programming makes sense except in the light of human psychology.}

This rule is a play on Theodosius Dobzhansky's famous dictum that nothing in
biology makes sense except in light of evolution \cite{Dobz1973}.  Computers
don't have to understand instructions in order to execute them, but we need to
understand them in order to create them (and fix them afterward).  Our brains
can only keep track of a handful of things at once \cite{Mill1956}, we have to
decompose complexity into chunks, then think in terms of those chunks.

One key to making abstraction work is to separate \emph{interface} and
\emph{implementation}.  An interface is what something knows how to do: the
questions it can answer, or the operations it can carry out. Its implementation
is how it does those things: what data it stores, what algorithms it uses, and
so on.  There can be dozens of ways to implement a particular interface; if we
do our work well, we shouldn't have to care about the implementation until
something goes wrong or we need to improve its performance.  ``Spare me the
details'' may be rude in real life, but it's essential in programming.

One useful rule of thumb is that every pattern is an abstraction trying to be
born.  Does your program repeatedly search an array to find the largest and
smallest values?  Write a function called \texttt{bounds}.  Does it repeatedly
search arbitrary data structures to find values that meet certain criteria?
Write a generator that returns values one by one and another function that
filters those according to some criteria.  The history of programming is in
large part the history of people noticing patterns and then making it easier
for programmers to work with them.

\rulemajor{Models are for computers, and views are for people.}

One implication of our need for abstractions is important enough to be its own
rule: create models for computers and views for people.  A \emph{model} is a
representation of something that is easy for a computer to operate on; a
\emph{view} is a way of displaying it that human beings can understand.  For
example, an HTML page consists of elements that have attributes and that contain
other elements or blocks of text:

\includegraphics[width=10.0cm]{modelview.png}

That model can be rendered in a browser, turned into speech for someone who is
visually impaired, or displayed as text using angle brackets, quotes, and some
indentation.  None of these \emph{is} the model: they're all views that make the
model's content accessible to human beings in different contexts.  The model
itself isn't just easier for the computer to work with: it's essential, since as
we said before, the computer can't ``see'' the views that we create for human
beings.

Turning a view back into a model is hard: parsing the textual representation of
HTML takes thousands of lines of code, and doing OCR or speech recognition to
translate the rendered page or its spoken equivalent can take millions.  What
this idea implies, therefore, is that \emph{structured data is better than
  unstructured data}.  The tags and attributes in the textual representation of
an HTML page are there because without it, the computer can't tell whether
something is in italics because it's being emphasized or because it's the title
of a book.  To borrow an example from Jon Udell, a PDF with a cartoon whose
caption says, ``The knitting circle meets on the second Tuesday of every month''
is a lot easier for human beings to understand than a calendar entry in iCal
format, but the second is much easier for the computer.

\rulemajor{Paranoia makes us productive.}

Even simple software can go wrong in a bewildering variety of ways.  Since
fixing something after it breaks takes more time than getting it right in the
first place, experienced programmers have learned the value of investing time up
front in figuring out exactly what they're trying to accomplish, and in ensuring
that what they've built meets those requirements.  ``I want to count the stars
in this photograph'' is easy to say, but what does it actually mean?  What
constitutes a star?  When do you decide that a lumpy blob of pixels is two stars
rather than one, or three instead of two?  Every program embodies decisions
about questions like these, even if you don't realize that there was a question
and that you made a choice.

The sooner these decisions are made explicit and the earlier they're checked,
the less time is wasted.  Recent research shows that fine-grained
development---i.e., interleaving short bursts of coding and testing---is more
effective than working in larger chunks \cite{Fucc2017}, since the objectives
and implementation of the code are fresh in the programmer's mind while testing.

Of course, we don't stop worrying once we've typed our code in.  We check that
data is formatted properly to protect ourselves against ``garbage in, garbage
out''.  We put checks in our code to make sure that parameters are sensible,
data structures consistent, files aren't empty, and so on.  This is called
\emph{defensive programming}, and one of the signs of a mature programmer is
a high density of assertions and other self-checks in her code.

\rulemajor{Better algorithms are better than better hardware.}

One of the greatest mathematical advances of the Twentieth Century was the idea
of \emph{algorithmic complexity}, and its practical implications shape
everything we do with computers, whether we realize it or not.  The basic idea
is that we can estimate how the running time or memory requirements of an
algorithm grow as a function of the size of the problem we're trying to solve
\cite{Cone2016}.  while some algorithms slow down gently as their inputs get
larger, others slow down so much that even if the whole universe was one large
computer, it couldn't solve any problem big enough to be interesting.  Faster
chips help---a lot---but the real key to speed is to focus on how we're doing
things, not what we're doing it with.

But algorithms are nothing without data structures to operate on, just as data
structures are pointless without algorithms to manipulate them.  That's why the
two topics are usually taught together: arrays with loops, trees with recursion,
and so on.  Knowing the syntax of this language or the API of that library is
useful, but good programmers know key data structures and algorithms the way a
musician knows scales.

\rulemajor{We can't break what we don't change.}

Programmers use the words \emph{mutable} and \emph{immutable} to refer to things
that can be modified after creation and things that can't.  Things that are
immutable are easier to understand: once a data structure has been defined,
nothing can change it, so you don't have to re-trace the history of a program's
execution in order to understand it.

However, immutable data can be less efficient than mutable data: it's very
expensive to make a copy of an entire multi-megabyte image just because we want
to change one pixel.  Older programming languages like C and Fortran allowed
most data to be mutable because computer time was expensive Other languages like
R and MATLAB do a lot of work to re-use data without changing it when they can,
and only alter the data that absolutely must be changed, and more modern
languages (and programming styles in older languages) strongly encourage
programmers to treat everything as immutable.

This rule applies to software as well (after all, our first rule was that
software is just another kind of data).  Experienced programmers write very
little software from scratch.  Instead, they re-use libraries that others have
developed.

Another realization of this rule is automating workflows.  As Whitehead said
\cite{Whit1958}, ``Civilization advances by extending the number of important
operations which we can perform without thinking about them.''  Every time we
automate a task, such as recompiling software or re-running a data analysis when
a new input file appears, we reduce the chances of getting it wrong the next
time, and have more time to think about things that machines \emph{can't} do for
us.

\rulemajor{You are not your users.}

``It's obvious,'' says the programmer. ``You just{\ldots}'' What follows is
often incomprehensible, because programmers frequently suffer from \emph{expert
  blind spot} \cite{Nath2003}.  An interface that makes sense to the developer
who built it may be frustrating and possibly dangerous in the hands of other
people---including other programmers.  Even if it's comprehensible, it may not
be useful, because the developer may not understand the domain well enough to
know what should be easy to do and what doesn't matter.  Being able to put
herself in someone else's place and see the world through their eyes is one of
the hallmarks of a good programmer.

\rulemajor{The street finds its own uses for things.}

This rule comes from William Gibson's short story ``Burning Chrome''
\cite{Gibs1987}.  If the last few years have taught programmers anything, it's
that every decision about what data to collect, who to share it with, how to
interpret it, and how to use those interpretations furthers someone's interests.
The same tools that are used to recommend music can be used to generate targeted
political misinformation; the monitoring done to customize online lessons can be
combined with crime data in ways that reinforce the biases already present in
both, and the odds are that the people using and abusing data in these ways will
never be held responsible for the decisions based on their analyses.  Like
security, accountability cannot be added as an afterthought.

But this rule is not entirely dire.  The biggest change in programming over the
last twenty years has been the rise of question-and-answer sites like Stack
Overflow.  Many programmers consult it hourly, particularly when they are
working in a new domain or struggling with something written by someone who
ignored our previous rule.  While developers usually don't just copy and paste,
almost all borrow and modify, often in ways that the original question and
answer could not have anticipated.  Knowing how to frame a good question and
recognize a useful answer is therefore now as important to software development
as it is to research.

\rulemajor{The tool shapes the hand.}

Our final rule is something that artisans have known for thousands of years:
\emph{the tool shapes the hand}.  Building software changes how you use
software; making computers do new things changes your understanding of what
computers can do.  That's why the only way to learn how to think like a
programmer is to do some programming: however frustrating it may sometimes be,
it's the only way to learn what software can do for you, and how it can change
your view of everything else you do.

\bibliography{rules}

\end{document}
